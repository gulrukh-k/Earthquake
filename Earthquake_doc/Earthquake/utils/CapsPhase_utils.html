<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 11.0.0"/>
    <title>Earthquake.utils.CapsPhase_utils API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent }nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .pdoc-alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:1rem center;margin-bottom:1rem;}.pdoc .pdoc-alert > *:last-child{margin-bottom:0;}.pdoc .pdoc-alert-note {color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .pdoc-alert-warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .pdoc-alert-danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--code);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(:first-of-type){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.pdoc details{filter:opacity(1);}.pdoc details:not([open]){height:0;}.pdoc details > summary{position:absolute;top:-35px;right:0;font-size:.75rem;color:var(--muted);padding:0 .7em;user-select:none;cursor:pointer;}.pdoc details > summary:focus{outline:0;}.pdoc > section:first-of-type details > summary{top:-20px;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc .headerlink{position:absolute;width:0;margin-left:-1.5rem;line-height:1.4rem;font-size:1.5rem;font-weight:normal;transition:all 100ms ease-in-out;opacity:0;user-select:none;}.pdoc .attr > .headerlink{margin-left:-2.5rem;}.pdoc *:hover > .headerlink,.pdoc *:target > .attr > .headerlink{opacity:1;}.pdoc .attr{display:block;color:var(--text);margin:.5rem 0 .5rem;padding:.4rem 5rem .4rem 1rem;background-color:var(--accent);}.pdoc .classattr{margin-left:2rem;}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{white-space:pre-wrap;}.pdoc .annotation{color:var(--annotation);}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>            <a class="pdoc-button module-list-button" href="../utils.html">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-box-arrow-in-left" viewBox="0 0 16 16">
  <path fill-rule="evenodd" d="M10 3.5a.5.5 0 0 0-.5-.5h-8a.5.5 0 0 0-.5.5v9a.5.5 0 0 0 .5.5h8a.5.5 0 0 0 .5-.5v-2a.5.5 0 0 1 1 0v2A1.5 1.5 0 0 1 9.5 14h-8A1.5 1.5 0 0 1 0 12.5v-9A1.5 1.5 0 0 1 1.5 2h8A1.5 1.5 0 0 1 11 3.5v2a.5.5 0 0 1-1 0v-2z"/>
  <path fill-rule="evenodd" d="M4.146 8.354a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H14.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3z"/>
</svg>                &nbsp;Earthquake.utils</a>


            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>



        <h2>API Documentation</h2>
            <ul class="memberlist">
            <li>
                    <a class="function" href="#own_batch_dot">own_batch_dot</a>
            </li>
            <li>
                    <a class="class" href="#Length">Length</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#Length.call">call</a>
                        </li>
                        <li>
                                <a class="function" href="#Length.compute_output_shape">compute_output_shape</a>
                        </li>
                        <li>
                                <a class="function" href="#Length.get_config">get_config</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#Mask">Mask</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#Mask.call">call</a>
                        </li>
                        <li>
                                <a class="function" href="#Mask.compute_output_shape">compute_output_shape</a>
                        </li>
                        <li>
                                <a class="function" href="#Mask.get_config">get_config</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="function" href="#squash">squash</a>
            </li>
            <li>
                    <a class="function" href="#margin_loss">margin_loss</a>
            </li>
            <li>
                    <a class="function" href="#plotfn">plotfn</a>
            </li>
            <li>
                    <a class="function" href="#butter_bandpass">butter_bandpass</a>
            </li>
            <li>
                    <a class="function" href="#butter_bandpass_filter_zi">butter_bandpass_filter_zi</a>
            </li>
            <li>
                    <a class="function" href="#butter_bandpass_filter">butter_bandpass_filter</a>
            </li>
            <li>
                    <a class="function" href="#sliding_window">sliding_window</a>
            </li>
            <li>
                    <a class="class" href="#CapsuleLayer">CapsuleLayer</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#CapsuleLayer.__init__">CapsuleLayer</a>
                        </li>
                        <li>
                                <a class="function" href="#CapsuleLayer.build">build</a>
                        </li>
                        <li>
                                <a class="function" href="#CapsuleLayer.call">call</a>
                        </li>
                        <li>
                                <a class="function" href="#CapsuleLayer.compute_output_shape">compute_output_shape</a>
                        </li>
                        <li>
                                <a class="function" href="#CapsuleLayer.get_config">get_config</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="function" href="#PrimaryCap">PrimaryCap</a>
            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section>
                    <h1 class="modulename">
<a href="./../../Earthquake.html">Earthquake</a><wbr>.<a href="./../utils.html">utils</a><wbr>.CapsPhase_utils    </h1>

                        <div class="docstring"><p>Some key layers used for constructing a Capsule Network. These layers can used to construct CapsNet on other dataset, 
not just on MNIST.
<em>NOTE</em>: some functions can be implemented in multiple ways, I keep all of them. You can try them for yourself just by
uncommenting them and commenting their counterparts.</p>

<p>Author: Xifeng Guo, E-mail: <code>guoxifeng1990@163.com</code>, Github: <code>https://github.com/XifengGuo/CapsNet-Keras</code></p>
</div>

                        <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Some key layers used for constructing a Capsule Network. These layers can used to construct CapsNet on other dataset, </span>
<span class="sd">not just on MNIST.</span>
<span class="sd">*NOTE*: some functions can be implemented in multiple ways, I keep all of them. You can try them for yourself just by</span>
<span class="sd">uncommenting them and commenting their counterparts.</span>

<span class="sd">Author: Xifeng Guo, E-mail: `guoxifeng1990@163.com`, Github: `https://github.com/XifengGuo/CapsNet-Keras`</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">keras.backend</span> <span class="k">as</span> <span class="nn">K</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">initializers</span><span class="p">,</span> <span class="n">layers</span>
<span class="c1">#from batchdot import own_batch_dot</span>
<span class="kn">from</span> <span class="nn">scipy.signal</span> <span class="kn">import</span> <span class="n">butter</span><span class="p">,</span> <span class="n">lfilter</span><span class="p">,</span> <span class="n">lfilter_zi</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">keras.backend</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">math_ops</span>


<span class="c1">#own_batch_dot = batch_dot  # force standard implementation </span>

<span class="c1"># import of batch_dot operation from TF 1.13</span>
<span class="c1"># https://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/python/keras/backend.py</span>

<span class="k">def</span> <span class="nf">own_batch_dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Batchwise dot product.</span>
<span class="sd">  `batch_dot` is used to compute dot product of `x` and `y` when</span>
<span class="sd">  `x` and `y` are data in batch, i.e. in a shape of</span>
<span class="sd">  `(batch_size, :)`.</span>
<span class="sd">  `batch_dot` results in a tensor or variable with less dimensions</span>
<span class="sd">  than the input. If the number of dimensions is reduced to 1,</span>
<span class="sd">  we use `expand_dims` to make sure that ndim is at least 2.</span>
<span class="sd">  Arguments:</span>
<span class="sd">      x: Keras tensor or variable with `ndim &gt;= 2`.</span>
<span class="sd">      y: Keras tensor or variable with `ndim &gt;= 2`.</span>
<span class="sd">      axes: list of (or single) int with target dimensions.</span>
<span class="sd">          The lengths of `axes[0]` and `axes[1]` should be the same.</span>
<span class="sd">  Returns:</span>
<span class="sd">      A tensor with shape equal to the concatenation of `x`&#39;s shape</span>
<span class="sd">      (less the dimension that was summed over) and `y`&#39;s shape</span>
<span class="sd">      (less the batch dimension and the dimension that was summed over).</span>
<span class="sd">      If the final rank is 1, we reshape it to `(batch_size, 1)`.</span>
<span class="sd">  Examples:</span>
<span class="sd">      Assume `x = [[1, 2], [3, 4]]` and `y = [[5, 6], [7, 8]]`</span>
<span class="sd">      `batch_dot(x, y, axes=1) = [[17, 53]]` which is the main diagonal</span>
<span class="sd">      of `x.dot(y.T)`, although we never have to calculate the off-diagonal</span>
<span class="sd">      elements.</span>
<span class="sd">      Shape inference:</span>
<span class="sd">      Let `x`&#39;s shape be `(100, 20)` and `y`&#39;s shape be `(100, 30, 20)`.</span>
<span class="sd">      If `axes` is (1, 2), to find the output shape of resultant tensor,</span>
<span class="sd">          loop through each dimension in `x`&#39;s shape and `y`&#39;s shape:</span>
<span class="sd">      * `x.shape[0]` : 100 : append to output shape</span>
<span class="sd">      * `x.shape[1]` : 20 : do not append to output shape,</span>
<span class="sd">          dimension 1 of `x` has been summed over. (`dot_axes[0]` = 1)</span>
<span class="sd">      * `y.shape[0]` : 100 : do not append to output shape,</span>
<span class="sd">          always ignore first dimension of `y`</span>
<span class="sd">      * `y.shape[1]` : 30 : append to output shape</span>
<span class="sd">      * `y.shape[2]` : 20 : do not append to output shape,</span>
<span class="sd">          dimension 2 of `y` has been summed over. (`dot_axes[1]` = 2)</span>
<span class="sd">      `output_shape` = `(100, 30)`</span>
<span class="sd">  ```python</span>
<span class="sd">      &gt;&gt;&gt; x_batch = K.ones(shape=(32, 20, 1))</span>
<span class="sd">      &gt;&gt;&gt; y_batch = K.ones(shape=(32, 30, 20))</span>
<span class="sd">      &gt;&gt;&gt; xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2])</span>
<span class="sd">      &gt;&gt;&gt; K.int_shape(xy_batch_dot)</span>
<span class="sd">      (32, 1, 30)</span>
<span class="sd">  ```</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
  <span class="n">x_ndim</span> <span class="o">=</span> <span class="n">ndim</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">y_ndim</span> <span class="o">=</span> <span class="n">ndim</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># behaves like tf.batch_matmul as default</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">y_ndim</span> <span class="o">-</span> <span class="mi">2</span><span class="p">]</span>
  <span class="k">if</span> <span class="n">x_ndim</span> <span class="o">&gt;</span> <span class="n">y_ndim</span><span class="p">:</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">x_ndim</span> <span class="o">-</span> <span class="n">y_ndim</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span>
                          <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
                              <span class="p">[</span><span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">diff</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
  <span class="k">elif</span> <span class="n">y_ndim</span> <span class="o">&gt;</span> <span class="n">x_ndim</span><span class="p">:</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">y_ndim</span> <span class="o">-</span> <span class="n">x_ndim</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span>
                          <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
                              <span class="p">[</span><span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">diff</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">if</span> <span class="n">ndim</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">ndim</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">math_ops</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
          <span class="n">math_ops</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">array_ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">y</span><span class="p">),</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">adj_x</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">ndim</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">True</span>
    <span class="n">adj_y</span> <span class="o">=</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">ndim</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">adjoint_a</span><span class="o">=</span><span class="n">adj_x</span><span class="p">,</span> <span class="n">adjoint_b</span><span class="o">=</span><span class="n">adj_y</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">diff</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">x_ndim</span> <span class="o">&gt;</span> <span class="n">y_ndim</span><span class="p">:</span>
      <span class="n">idx</span> <span class="o">=</span> <span class="n">x_ndim</span> <span class="o">+</span> <span class="n">y_ndim</span> <span class="o">-</span> <span class="mi">3</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">idx</span> <span class="o">=</span> <span class="n">x_ndim</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span> <span class="o">+</span> <span class="n">diff</span><span class="p">)))</span>
  <span class="k">if</span> <span class="n">ndim</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">expand_dims</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">out</span>

<span class="k">class</span> <span class="nc">Length</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss.</span>
<span class="sd">    Using this layer as model&#39;s output can directly predict labels by using `y_pred = np.argmax(model.predict(x), 1)`</span>
<span class="sd">    inputs: shape=[None, num_vectors, dim_vector]</span>
<span class="sd">    output: shape=[None, num_vectors]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">K</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">K</span><span class="o">.</span><span class="n">epsilon</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">Length</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">config</span>


<span class="k">class</span> <span class="nc">Mask</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Mask a Tensor with shape=[None, num_capsule, dim_vector] either by the capsule with max length or by an additional </span>
<span class="sd">    input mask. Except the max-length capsule (or specified capsule), all vectors are masked to zeros. Then flatten the</span>
<span class="sd">    masked Tensor.</span>
<span class="sd">    For example:</span>
<span class="sd">        ```</span>
<span class="sd">        x = keras.layers.Input(shape=[8, 3, 2])  # batch_size=8, each sample contains 3 capsules with dim_vector=2</span>
<span class="sd">        y = keras.layers.Input(shape=[8, 3])  # True labels. 8 samples, 3 classes, one-hot coding.</span>
<span class="sd">        out = Mask()(x)  # out.shape=[8, 6]</span>
<span class="sd">        # or</span>
<span class="sd">        out2 = Mask()([x, y])  # out2.shape=[8,6]. Masked with true labels y. Of course y can also be manipulated.</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">list</span><span class="p">:</span>  <span class="c1"># true label is provided with shape = [None, n_classes], i.e. one-hot code.</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># if no true label, mask by the max length of capsules. Mainly used for prediction</span>
            <span class="c1"># compute lengths of capsules</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="c1"># generate the mask which is a one-hot code.</span>
            <span class="c1"># mask.shape=[None, n_classes]=[None, num_capsule]</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">K</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># inputs.shape=[None, num_capsule, dim_capsule]</span>
        <span class="c1"># mask.shape=[None, num_capsule]</span>
        <span class="c1"># masked.shape=[None, num_capsule * dim_capsule]</span>
        <span class="n">masked</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">batch_flatten</span><span class="p">(</span><span class="n">inputs</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">masked</span>

    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">is</span> <span class="nb">tuple</span><span class="p">:</span>  <span class="c1"># true label provided</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]])</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># no true label provided</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]])</span>

    <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">Mask</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">config</span>


<span class="k">def</span> <span class="nf">squash</span><span class="p">(</span><span class="n">vectors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0</span>
<span class="sd">    :param vectors: some vectors to be squashed, N-dim tensor</span>
<span class="sd">    :param axis: the axis to squash</span>
<span class="sd">    :return: a Tensor with same shape as input vectors</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">s_squared_norm</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">vectors</span><span class="p">),</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">s_squared_norm</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">s_squared_norm</span><span class="p">)</span> <span class="o">/</span> <span class="n">K</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s_squared_norm</span> <span class="o">+</span> <span class="n">K</span><span class="o">.</span><span class="n">epsilon</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">vectors</span>

<span class="k">def</span> <span class="nf">margin_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>

    <span class="n">L</span> <span class="o">=</span> <span class="n">y_true</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.9</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">))</span> <span class="o">+</span> \
    <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">-</span> <span class="mf">0.1</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">K</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="c1"># Plotting Function.</span>
<span class="k">def</span> <span class="nf">plotfn</span><span class="p">(</span><span class="n">prob_P</span><span class="p">,</span><span class="n">prob_S</span><span class="p">):</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span> <span class="p">:</span> <span class="s1">&#39;normal&#39;</span><span class="p">,</span>
        <span class="s1">&#39;weight&#39;</span> <span class="p">:</span> <span class="s1">&#39;bold&#39;</span><span class="p">,</span>
        <span class="s1">&#39;size&#39;</span>   <span class="p">:</span> <span class="mi">16</span><span class="p">}</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">&#39;font&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">font</span><span class="p">)</span>

    <span class="n">yint</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">labf</span><span class="o">=</span><span class="n">prob_P</span>
    <span class="n">labfx</span><span class="o">=</span><span class="n">prob_S</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Output P-Probability&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="s1">&#39;large&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="c1">#ax1.plot(out,linewidth=2)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">labf</span><span class="p">,</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1">#ax1.set_xlim(0,len(out))</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">labf</span><span class="p">))</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1">#plt.yticks(yint)</span>


    <span class="n">ax2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">labfx</span><span class="p">,</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Window Index&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="s1">&#39;large&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Output S-Probability&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="s1">&#39;large&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="c1">#ax2.set_xlim(0,len(out))</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">labfx</span><span class="p">))</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">butter_bandpass</span><span class="p">(</span><span class="n">lowcut</span><span class="p">,</span> <span class="n">highcut</span><span class="p">,</span> <span class="n">fs</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">nyq</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">fs</span>
    <span class="n">low</span> <span class="o">=</span> <span class="n">lowcut</span> <span class="o">/</span> <span class="n">nyq</span>
    <span class="n">high</span> <span class="o">=</span> <span class="n">highcut</span> <span class="o">/</span> <span class="n">nyq</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="n">butter</span><span class="p">(</span><span class="n">order</span><span class="p">,</span> <span class="p">[</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">],</span> <span class="n">btype</span><span class="o">=</span><span class="s1">&#39;band&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">b</span><span class="p">,</span> <span class="n">a</span>
<span class="k">def</span> <span class="nf">butter_bandpass_filter_zi</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">lowcut</span><span class="p">,</span> <span class="n">highcut</span><span class="p">,</span> <span class="n">fs</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="n">butter_bandpass</span><span class="p">(</span><span class="n">lowcut</span><span class="p">,</span> <span class="n">highcut</span><span class="p">,</span> <span class="n">fs</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="n">order</span><span class="p">)</span>
    <span class="n">zi</span> <span class="o">=</span> <span class="n">lfilter_zi</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
    <span class="n">y</span><span class="p">,</span><span class="n">zo</span> <span class="o">=</span> <span class="n">lfilter</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">zi</span><span class="o">=</span><span class="n">zi</span><span class="o">*</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">y</span>
<span class="k">def</span> <span class="nf">butter_bandpass_filter</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">lowcut</span><span class="p">,</span> <span class="n">highcut</span><span class="p">,</span> <span class="n">fs</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">nyq</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">fs</span>
    <span class="n">low</span> <span class="o">=</span> <span class="n">lowcut</span> <span class="o">/</span> <span class="n">nyq</span>
    <span class="n">high</span> <span class="o">=</span> <span class="n">highcut</span> <span class="o">/</span> <span class="n">nyq</span>

    <span class="n">b</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="n">butter</span><span class="p">(</span><span class="n">order</span><span class="p">,</span> <span class="p">[</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">],</span> <span class="n">btype</span><span class="o">=</span><span class="s1">&#39;band&#39;</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">lfilter</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>
	
<span class="k">def</span> <span class="nf">sliding_window</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">stepsize</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padded</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate a sliding window over a signal</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    data : numpy array</span>
<span class="sd">        The array to be slided over.</span>
<span class="sd">    size : int</span>
<span class="sd">        The sliding window size</span>
<span class="sd">    stepsize : int</span>
<span class="sd">        The sliding window stepsize. Defaults to 1.</span>
<span class="sd">    axis : int</span>
<span class="sd">        The axis to slide over. Defaults to the last axis.</span>
<span class="sd">    copy : bool</span>
<span class="sd">        Return strided array as copy to avoid sideffects when manipulating the</span>
<span class="sd">        output array.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    data : numpy array</span>
<span class="sd">        A matrix where row in last dimension consists of one instance</span>
<span class="sd">        of the sliding window.</span>
<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    - Be wary of setting `copy` to `False` as undesired sideffects with the</span>
<span class="sd">      output values may occurr.</span>
<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; a = numpy.array([1, 2, 3, 4, 5])</span>
<span class="sd">    &gt;&gt;&gt; sliding_window(a, size=3)</span>
<span class="sd">    array([[1, 2, 3],</span>
<span class="sd">           [2, 3, 4],</span>
<span class="sd">           [3, 4, 5]])</span>
<span class="sd">    &gt;&gt;&gt; sliding_window(a, size=3, stepsize=2)</span>
<span class="sd">    array([[1, 2, 3],</span>
<span class="sd">           [3, 4, 5]])</span>
<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    pieces : Calculate number of pieces available by sliding</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">&gt;=</span> <span class="n">data</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Axis value out of range&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">stepsize</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Stepsize may not be zero or negative&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">size</span> <span class="o">&gt;</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Sliding window size may not exceed size of selected axis&quot;</span>
        <span class="p">)</span>

    <span class="n">shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">/</span> <span class="n">stepsize</span> <span class="o">-</span> <span class="n">size</span> <span class="o">/</span> <span class="n">stepsize</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>

    <span class="n">strides</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">strides</span><span class="p">)</span>
    <span class="n">strides</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">*=</span> <span class="n">stepsize</span>
    <span class="n">strides</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="n">axis</span><span class="p">])</span>

    <span class="n">strided</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">lib</span><span class="o">.</span><span class="n">stride_tricks</span><span class="o">.</span><span class="n">as_strided</span><span class="p">(</span>
        <span class="n">data</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="n">strides</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">copy</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">strided</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">strided</span>
	
<span class="k">class</span> <span class="nc">CapsuleLayer</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the </span>
<span class="sd">    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron</span>
<span class="sd">    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \</span>
<span class="sd">    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.</span>
<span class="sd">    </span>
<span class="sd">    :param num_capsule: number of capsules in this layer</span>
<span class="sd">    :param dim_capsule: dimension of the output vectors of the capsules in this layer</span>
<span class="sd">    :param routings: number of iterations for the routing algorithm</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_capsule</span><span class="p">,</span> <span class="n">dim_capsule</span><span class="p">,</span> <span class="n">routings</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                 <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;glorot_uniform&#39;</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CapsuleLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_capsule</span> <span class="o">=</span> <span class="n">num_capsule</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim_capsule</span> <span class="o">=</span> <span class="n">dim_capsule</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">routings</span> <span class="o">=</span> <span class="n">routings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_initializer</span> <span class="o">=</span> <span class="n">initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">kernel_initializer</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_num_capsule</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim_capsule</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

        <span class="c1"># Transform matrix</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_capsule</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_num_capsule</span><span class="p">,</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">dim_capsule</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim_capsule</span><span class="p">],</span>
                                 <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_initializer</span><span class="p">,</span>
                                 <span class="n">name</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># inputs.shape=[None, input_num_capsule, input_dim_capsule]</span>
        <span class="c1"># inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]</span>
        <span class="n">inputs_expand</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Replicate num_capsule dimension to prepare being multiplied by W</span>
        <span class="c1"># inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule]</span>
        <span class="n">inputs_tiled</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">inputs_expand</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_capsule</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

        <span class="c1"># Compute `inputs * W` by scanning inputs_tiled on dimension 0.</span>
        <span class="c1"># x.shape=[num_capsule, input_num_capsule, input_dim_capsule]</span>
        <span class="c1"># W.shape=[num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]</span>
        <span class="c1"># Regard the first two dimensions as `batch` dimension,</span>
        <span class="c1"># then matmul: [input_dim_capsule] x [dim_capsule, input_dim_capsule]^T -&gt; [dim_capsule].</span>
        <span class="c1"># inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]</span>
        <span class="n">inputs_hat</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">map_fn</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">own_batch_dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">elems</span><span class="o">=</span><span class="n">inputs_tiled</span><span class="p">)</span>

        <span class="c1"># Begin: Routing algorithm ---------------------------------------------------------------------#</span>
        <span class="c1"># The prior for coupling coefficient, initialized as zeros.</span>
        <span class="c1"># b.shape = [None, self.num_capsule, self.input_num_capsule].</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs_hat</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_capsule</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_num_capsule</span><span class="p">])</span>

        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">routings</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;The routings should be &gt; 0.&#39;</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">routings</span><span class="p">):</span>
            <span class="c1"># c.shape=[batch_size, num_capsule, input_num_capsule]</span>
            <span class="n">c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># c.shape =  [batch_size, num_capsule, input_num_capsule]</span>
            <span class="c1"># inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]</span>
            <span class="c1"># The first two dimensions as `batch` dimension,</span>
            <span class="c1"># then matmal: [input_num_capsule] x [input_num_capsule, dim_capsule] -&gt; [dim_capsule].</span>
            <span class="c1"># outputs.shape=[None, num_capsule, dim_capsule]</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">squash</span><span class="p">(</span><span class="n">own_batch_dot</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">inputs_hat</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))</span>  <span class="c1"># [None, 10, 16]</span>
			<span class="c1">#print(c,inputs_hat,outputs)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">routings</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># outputs.shape =  [None, num_capsule, dim_capsule]</span>
                <span class="c1"># inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]</span>
                <span class="c1"># The first two dimensions as `batch` dimension,</span>
                <span class="c1"># then matmal: [dim_capsule] x [input_num_capsule, dim_capsule]^T -&gt; [input_num_capsule].</span>
                <span class="c1"># b.shape=[batch_size, num_capsule, input_num_capsule]</span>
                <span class="n">b</span> <span class="o">+=</span> <span class="n">own_batch_dot</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">inputs_hat</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
        <span class="c1"># End: Routing algorithm -----------------------------------------------------------------------#</span>

        <span class="k">return</span> <span class="n">outputs</span>

    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_capsule</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_capsule</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;num_capsule&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_capsule</span><span class="p">,</span>
            <span class="s1">&#39;dim_capsule&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_capsule</span><span class="p">,</span>
            <span class="s1">&#39;routings&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">routings</span>
        <span class="p">}</span>
        <span class="n">base_config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">CapsuleLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">base_config</span><span class="o">.</span><span class="n">items</span><span class="p">())</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">items</span><span class="p">()))</span>


<span class="k">def</span> <span class="nf">PrimaryCap</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">dim_capsule</span><span class="p">,</span> <span class="n">n_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">padding</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Apply Conv2D `n_channels` times and concatenate all capsules</span>
<span class="sd">    :param inputs: 4D tensor, shape=[None, width, height, channels]</span>
<span class="sd">    :param dim_capsule: the dim of the output vector of capsule</span>
<span class="sd">    :param n_channels: the number of types of capsules</span>
<span class="sd">    :return: output tensor, shape=[None, num_capsule, dim_capsule]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="n">dim_capsule</span><span class="o">*</span><span class="n">n_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                           <span class="n">name</span><span class="o">=</span><span class="s1">&#39;primarycap_conv2d&#39;</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Reshape</span><span class="p">(</span><span class="n">target_shape</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim_capsule</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;primarycap_reshape&#39;</span><span class="p">)(</span><span class="n">output</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">layers</span><span class="o">.</span><span class="n">Lambda</span><span class="p">(</span><span class="n">squash</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;primarycap_squash&#39;</span><span class="p">)(</span><span class="n">outputs</span><span class="p">)</span>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd"># The following is another way to implement primary capsule layer. This is much slower.</span>
<span class="sd"># Apply Conv2D `n_channels` times and concatenate all capsules</span>
<span class="sd">def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):</span>
<span class="sd">    outputs = []</span>
<span class="sd">    for _ in range(n_channels):</span>
<span class="sd">        output = layers.Conv2D(filters=dim_capsule, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)</span>
<span class="sd">        outputs.append(layers.Reshape([output.get_shape().as_list()[1] ** 2, dim_capsule])(output))</span>
<span class="sd">    outputs = layers.Concatenate(axis=1)(outputs)</span>
<span class="sd">    return layers.Lambda(squash)(outputs)</span>
<span class="sd">&quot;&quot;&quot;</span>
</pre></div>

        </details>

            </section>
                <section id="own_batch_dot">
                            <div class="attr function"><a class="headerlink" href="#own_batch_dot">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">own_batch_dot</span><span class="signature">(x, y, axes=None)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">def</span> <span class="nf">own_batch_dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Batchwise dot product.</span>
<span class="sd">  `batch_dot` is used to compute dot product of `x` and `y` when</span>
<span class="sd">  `x` and `y` are data in batch, i.e. in a shape of</span>
<span class="sd">  `(batch_size, :)`.</span>
<span class="sd">  `batch_dot` results in a tensor or variable with less dimensions</span>
<span class="sd">  than the input. If the number of dimensions is reduced to 1,</span>
<span class="sd">  we use `expand_dims` to make sure that ndim is at least 2.</span>
<span class="sd">  Arguments:</span>
<span class="sd">      x: Keras tensor or variable with `ndim &gt;= 2`.</span>
<span class="sd">      y: Keras tensor or variable with `ndim &gt;= 2`.</span>
<span class="sd">      axes: list of (or single) int with target dimensions.</span>
<span class="sd">          The lengths of `axes[0]` and `axes[1]` should be the same.</span>
<span class="sd">  Returns:</span>
<span class="sd">      A tensor with shape equal to the concatenation of `x`&#39;s shape</span>
<span class="sd">      (less the dimension that was summed over) and `y`&#39;s shape</span>
<span class="sd">      (less the batch dimension and the dimension that was summed over).</span>
<span class="sd">      If the final rank is 1, we reshape it to `(batch_size, 1)`.</span>
<span class="sd">  Examples:</span>
<span class="sd">      Assume `x = [[1, 2], [3, 4]]` and `y = [[5, 6], [7, 8]]`</span>
<span class="sd">      `batch_dot(x, y, axes=1) = [[17, 53]]` which is the main diagonal</span>
<span class="sd">      of `x.dot(y.T)`, although we never have to calculate the off-diagonal</span>
<span class="sd">      elements.</span>
<span class="sd">      Shape inference:</span>
<span class="sd">      Let `x`&#39;s shape be `(100, 20)` and `y`&#39;s shape be `(100, 30, 20)`.</span>
<span class="sd">      If `axes` is (1, 2), to find the output shape of resultant tensor,</span>
<span class="sd">          loop through each dimension in `x`&#39;s shape and `y`&#39;s shape:</span>
<span class="sd">      * `x.shape[0]` : 100 : append to output shape</span>
<span class="sd">      * `x.shape[1]` : 20 : do not append to output shape,</span>
<span class="sd">          dimension 1 of `x` has been summed over. (`dot_axes[0]` = 1)</span>
<span class="sd">      * `y.shape[0]` : 100 : do not append to output shape,</span>
<span class="sd">          always ignore first dimension of `y`</span>
<span class="sd">      * `y.shape[1]` : 30 : append to output shape</span>
<span class="sd">      * `y.shape[2]` : 20 : do not append to output shape,</span>
<span class="sd">          dimension 2 of `y` has been summed over. (`dot_axes[1]` = 2)</span>
<span class="sd">      `output_shape` = `(100, 30)`</span>
<span class="sd">  ```python</span>
<span class="sd">      &gt;&gt;&gt; x_batch = K.ones(shape=(32, 20, 1))</span>
<span class="sd">      &gt;&gt;&gt; y_batch = K.ones(shape=(32, 30, 20))</span>
<span class="sd">      &gt;&gt;&gt; xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2])</span>
<span class="sd">      &gt;&gt;&gt; K.int_shape(xy_batch_dot)</span>
<span class="sd">      (32, 1, 30)</span>
<span class="sd">  ```</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
  <span class="n">x_ndim</span> <span class="o">=</span> <span class="n">ndim</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">y_ndim</span> <span class="o">=</span> <span class="n">ndim</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># behaves like tf.batch_matmul as default</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">y_ndim</span> <span class="o">-</span> <span class="mi">2</span><span class="p">]</span>
  <span class="k">if</span> <span class="n">x_ndim</span> <span class="o">&gt;</span> <span class="n">y_ndim</span><span class="p">:</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">x_ndim</span> <span class="o">-</span> <span class="n">y_ndim</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span>
                          <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
                              <span class="p">[</span><span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">diff</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
  <span class="k">elif</span> <span class="n">y_ndim</span> <span class="o">&gt;</span> <span class="n">x_ndim</span><span class="p">:</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">y_ndim</span> <span class="o">-</span> <span class="n">x_ndim</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span>
                          <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
                              <span class="p">[</span><span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">diff</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">if</span> <span class="n">ndim</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">ndim</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">math_ops</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
          <span class="n">math_ops</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">array_ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">y</span><span class="p">),</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">adj_x</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">ndim</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">True</span>
    <span class="n">adj_y</span> <span class="o">=</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">ndim</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">adjoint_a</span><span class="o">=</span><span class="n">adj_x</span><span class="p">,</span> <span class="n">adjoint_b</span><span class="o">=</span><span class="n">adj_y</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">diff</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">x_ndim</span> <span class="o">&gt;</span> <span class="n">y_ndim</span><span class="p">:</span>
      <span class="n">idx</span> <span class="o">=</span> <span class="n">x_ndim</span> <span class="o">+</span> <span class="n">y_ndim</span> <span class="o">-</span> <span class="mi">3</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">idx</span> <span class="o">=</span> <span class="n">x_ndim</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span> <span class="o">+</span> <span class="n">diff</span><span class="p">)))</span>
  <span class="k">if</span> <span class="n">ndim</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">expand_dims</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">out</span>
</pre></div>

        </details>

            <div class="docstring"><p>Batchwise dot product.
<code>batch_dot</code> is used to compute dot product of <code>x</code> and <code>y</code> when
<code>x</code> and <code>y</code> are data in batch, i.e. in a shape of
<code>(batch_size, :)</code>.
<code>batch_dot</code> results in a tensor or variable with less dimensions
than the input. If the number of dimensions is reduced to 1,
we use <code>expand_dims</code> to make sure that ndim is at least 2.
Arguments:
    x: Keras tensor or variable with <code>ndim &gt;= 2</code>.
    y: Keras tensor or variable with <code>ndim &gt;= 2</code>.
    axes: list of (or single) int with target dimensions.
        The lengths of <code>axes[0]</code> and <code>axes[1]</code> should be the same.
Returns:
    A tensor with shape equal to the concatenation of <code>x</code>'s shape
    (less the dimension that was summed over) and <code>y</code>'s shape
    (less the batch dimension and the dimension that was summed over).
    If the final rank is 1, we reshape it to <code>(batch_size, 1)</code>.
Examples:
    Assume <code>x = [[1, 2], [3, 4]]</code> and <code>y = [[5, 6], [7, 8]]</code>
    <code>batch_dot(x, y, axes=1) = [[17, 53]]</code> which is the main diagonal
    of <code>x.dot(y.T)</code>, although we never have to calculate the off-diagonal
    elements.
    Shape inference:
    Let <code>x</code>'s shape be <code>(100, 20)</code> and <code>y</code>'s shape be <code>(100, 30, 20)</code>.
    If <code>axes</code> is (1, 2), to find the output shape of resultant tensor,
        loop through each dimension in <code>x</code>'s shape and <code>y</code>'s shape:
    * <code>x.shape[0]</code> : 100 : append to output shape
    * <code>x.shape[1]</code> : 20 : do not append to output shape,
        dimension 1 of <code>x</code> has been summed over. (<code>dot_axes[0]</code> = 1)
    * <code>y.shape[0]</code> : 100 : do not append to output shape,
        always ignore first dimension of <code>y</code>
    * <code>y.shape[1]</code> : 30 : append to output shape
    * <code>y.shape[2]</code> : 20 : do not append to output shape,
        dimension 2 of <code>y</code> has been summed over. (<code>dot_axes[1]</code> = 2)
    <code>output_shape</code> = <code>(100, 30)</code></p>

<div class="pdoc-code codehilite"><pre><span></span><code>    <span class="o">&gt;&gt;&gt;</span> <span class="n">x_batch</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="o">&gt;&gt;&gt;</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
    <span class="o">&gt;&gt;&gt;</span> <span class="n">xy_batch_dot</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">batch_dot</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="o">&gt;&gt;&gt;</span> <span class="n">K</span><span class="o">.</span><span class="n">int_shape</span><span class="p">(</span><span class="n">xy_batch_dot</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
</code></pre></div>
</div>


                </section>
                <section id="Length">
                                <div class="attr class">
        <a class="headerlink" href="#Length">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">Length</span><wbr>(<span class="base">keras.engine.base_layer.Layer</span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">class</span> <span class="nc">Length</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss.</span>
<span class="sd">    Using this layer as model&#39;s output can directly predict labels by using `y_pred = np.argmax(model.predict(x), 1)`</span>
<span class="sd">    inputs: shape=[None, num_vectors, dim_vector]</span>
<span class="sd">    output: shape=[None, num_vectors]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">K</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">K</span><span class="o">.</span><span class="n">epsilon</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">Length</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">config</span>
</pre></div>

        </details>

            <div class="docstring"><p>Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss.
Using this layer as model's output can directly predict labels by using <code>y_pred = np.argmax(model.predict(x), 1)</code>
inputs: shape=[None, num_vectors, dim_vector]
output: shape=[None, num_vectors]</p>
</div>


                            <div id="Length.call" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#Length.call">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">call</span><span class="signature">(self, inputs, **kwargs)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">K</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">K</span><span class="o">.</span><span class="n">epsilon</span><span class="p">())</span>
</pre></div>

        </details>

            <div class="docstring"><p>This is where the layer's logic lives.</p>

<p>Note here that <code><a href="#Length.call">call()</a></code> method in <code>tf.keras</code> is little bit different
from <code>keras</code> API. In <code>keras</code> API, you can pass support masking for
layers as additional arguments. Whereas <code>tf.keras</code> has <code><a href="#Length.compute_mask">compute_mask()</a></code>
method to support masking.</p>

<p>Args:
    inputs: Input tensor, or list/tuple of input tensors.
    <em>args: Additional positional arguments. Currently unused.
    *</em>kwargs: Additional keyword arguments. Currently unused.</p>

<p>Returns:
    A tensor or list/tuple of tensors.</p>
</div>


                            </div>
                            <div id="Length.compute_output_shape" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#Length.compute_output_shape">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">compute_output_shape</span><span class="signature">(self, input_shape)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>

        </details>

            <div class="docstring"><p>Computes the output shape of the layer.</p>

<p>If the layer has not been built, this method will call <code><a href="#Length.build">build</a></code> on the
layer. This assumes that the layer will later be used with inputs that
match the input shape provided here.</p>

<p>Args:
    input_shape: Shape tuple (tuple of integers)
        or list of shape tuples (one per output tensor of the layer).
        Shape tuples can include None for free dimensions,
        instead of an integer.</p>

<p>Returns:
    An input shape tuple.</p>
</div>


                            </div>
                            <div id="Length.get_config" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#Length.get_config">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">get_config</span><span class="signature">(self)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">Length</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">config</span>
</pre></div>

        </details>

            <div class="docstring"><p>Returns the config of the layer.</p>

<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>

<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>

<p>Note that <code><a href="#Length.get_config">get_config()</a></code> does not guarantee to return a fresh copy of dict
every time it is called. The callers should make a copy of the returned dict
if they want to modify it.</p>

<p>Returns:
    Python dictionary.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>keras.engine.base_layer.Layer</dt>
                                <dd id="Length.__init__" class="function">Layer</dd>
                <dd id="Length.build" class="function">build</dd>
                <dd id="Length.add_weight" class="function">add_weight</dd>
                <dd id="Length.from_config" class="function">from_config</dd>
                <dd id="Length.compute_output_signature" class="function">compute_output_signature</dd>
                <dd id="Length.compute_mask" class="function">compute_mask</dd>
                <dd id="Length.dtype" class="variable">dtype</dd>
                <dd id="Length.name" class="variable">name</dd>
                <dd id="Length.supports_masking" class="variable">supports_masking</dd>
                <dd id="Length.dynamic" class="variable">dynamic</dd>
                <dd id="Length.stateful" class="variable">stateful</dd>
                <dd id="Length.trainable" class="variable">trainable</dd>
                <dd id="Length.activity_regularizer" class="variable">activity_regularizer</dd>
                <dd id="Length.input_spec" class="variable">input_spec</dd>
                <dd id="Length.trainable_weights" class="variable">trainable_weights</dd>
                <dd id="Length.non_trainable_weights" class="variable">non_trainable_weights</dd>
                <dd id="Length.weights" class="variable">weights</dd>
                <dd id="Length.updates" class="variable">updates</dd>
                <dd id="Length.losses" class="variable">losses</dd>
                <dd id="Length.add_loss" class="function">add_loss</dd>
                <dd id="Length.metrics" class="variable">metrics</dd>
                <dd id="Length.add_metric" class="function">add_metric</dd>
                <dd id="Length.add_update" class="function">add_update</dd>
                <dd id="Length.set_weights" class="function">set_weights</dd>
                <dd id="Length.get_weights" class="function">get_weights</dd>
                <dd id="Length.get_updates_for" class="function">get_updates_for</dd>
                <dd id="Length.get_losses_for" class="function">get_losses_for</dd>
                <dd id="Length.get_input_mask_at" class="function">get_input_mask_at</dd>
                <dd id="Length.get_output_mask_at" class="function">get_output_mask_at</dd>
                <dd id="Length.input_mask" class="variable">input_mask</dd>
                <dd id="Length.output_mask" class="variable">output_mask</dd>
                <dd id="Length.get_input_shape_at" class="function">get_input_shape_at</dd>
                <dd id="Length.get_output_shape_at" class="function">get_output_shape_at</dd>
                <dd id="Length.get_input_at" class="function">get_input_at</dd>
                <dd id="Length.get_output_at" class="function">get_output_at</dd>
                <dd id="Length.input" class="variable">input</dd>
                <dd id="Length.output" class="variable">output</dd>
                <dd id="Length.input_shape" class="variable">input_shape</dd>
                <dd id="Length.count_params" class="function">count_params</dd>
                <dd id="Length.output_shape" class="variable">output_shape</dd>
                <dd id="Length.inbound_nodes" class="variable">inbound_nodes</dd>
                <dd id="Length.outbound_nodes" class="variable">outbound_nodes</dd>
                <dd id="Length.apply" class="function">apply</dd>
                <dd id="Length.add_variable" class="function">add_variable</dd>
                <dd id="Length.variables" class="variable">variables</dd>
                <dd id="Length.trainable_variables" class="variable">trainable_variables</dd>
                <dd id="Length.non_trainable_variables" class="variable">non_trainable_variables</dd>
                <dd id="Length.dtype_policy" class="variable">dtype_policy</dd>
                <dd id="Length.compute_dtype" class="variable">compute_dtype</dd>
                <dd id="Length.variable_dtype" class="variable">variable_dtype</dd>

            </div>
            <div><dt>tensorflow.python.module.module.Module</dt>
                                <dd id="Length.name_scope" class="variable">name_scope</dd>
                <dd id="Length.submodules" class="variable">submodules</dd>
                <dd id="Length.with_name_scope" class="function">with_name_scope</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="Mask">
                                <div class="attr class">
        <a class="headerlink" href="#Mask">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">Mask</span><wbr>(<span class="base">keras.engine.base_layer.Layer</span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">class</span> <span class="nc">Mask</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Mask a Tensor with shape=[None, num_capsule, dim_vector] either by the capsule with max length or by an additional </span>
<span class="sd">    input mask. Except the max-length capsule (or specified capsule), all vectors are masked to zeros. Then flatten the</span>
<span class="sd">    masked Tensor.</span>
<span class="sd">    For example:</span>
<span class="sd">        ```</span>
<span class="sd">        x = keras.layers.Input(shape=[8, 3, 2])  # batch_size=8, each sample contains 3 capsules with dim_vector=2</span>
<span class="sd">        y = keras.layers.Input(shape=[8, 3])  # True labels. 8 samples, 3 classes, one-hot coding.</span>
<span class="sd">        out = Mask()(x)  # out.shape=[8, 6]</span>
<span class="sd">        # or</span>
<span class="sd">        out2 = Mask()([x, y])  # out2.shape=[8,6]. Masked with true labels y. Of course y can also be manipulated.</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">list</span><span class="p">:</span>  <span class="c1"># true label is provided with shape = [None, n_classes], i.e. one-hot code.</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># if no true label, mask by the max length of capsules. Mainly used for prediction</span>
            <span class="c1"># compute lengths of capsules</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="c1"># generate the mask which is a one-hot code.</span>
            <span class="c1"># mask.shape=[None, n_classes]=[None, num_capsule]</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">K</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># inputs.shape=[None, num_capsule, dim_capsule]</span>
        <span class="c1"># mask.shape=[None, num_capsule]</span>
        <span class="c1"># masked.shape=[None, num_capsule * dim_capsule]</span>
        <span class="n">masked</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">batch_flatten</span><span class="p">(</span><span class="n">inputs</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">masked</span>

    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">is</span> <span class="nb">tuple</span><span class="p">:</span>  <span class="c1"># true label provided</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]])</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># no true label provided</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]])</span>

    <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">Mask</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">config</span>
</pre></div>

        </details>

            <div class="docstring"><p>Mask a Tensor with shape=[None, num_capsule, dim_vector] either by the capsule with max length or by an additional 
input mask. Except the max-length capsule (or specified capsule), all vectors are masked to zeros. Then flatten the
masked Tensor.
For example:
    <code>
    x = keras.layers.Input(shape=[8, 3, 2])  # batch_size=8, each sample contains 3 capsules with dim_vector=2
    y = keras.layers.Input(shape=[8, 3])  # True labels. 8 samples, 3 classes, one-hot coding.
    out = Mask()(x)  # out.shape=[8, 6]
    # or
    out2 = Mask()([x, y])  # out2.shape=[8,6]. Masked with true labels y. Of course y can also be manipulated.
</code></p>
</div>


                            <div id="Mask.call" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#Mask.call">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">call</span><span class="signature">(self, inputs, **kwargs)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">list</span><span class="p">:</span>  <span class="c1"># true label is provided with shape = [None, n_classes], i.e. one-hot code.</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># if no true label, mask by the max length of capsules. Mainly used for prediction</span>
            <span class="c1"># compute lengths of capsules</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="c1"># generate the mask which is a one-hot code.</span>
            <span class="c1"># mask.shape=[None, n_classes]=[None, num_capsule]</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">K</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># inputs.shape=[None, num_capsule, dim_capsule]</span>
        <span class="c1"># mask.shape=[None, num_capsule]</span>
        <span class="c1"># masked.shape=[None, num_capsule * dim_capsule]</span>
        <span class="n">masked</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">batch_flatten</span><span class="p">(</span><span class="n">inputs</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">masked</span>
</pre></div>

        </details>

            <div class="docstring"><p>This is where the layer's logic lives.</p>

<p>Note here that <code><a href="#Mask.call">call()</a></code> method in <code>tf.keras</code> is little bit different
from <code>keras</code> API. In <code>keras</code> API, you can pass support masking for
layers as additional arguments. Whereas <code>tf.keras</code> has <code><a href="#Mask.compute_mask">compute_mask()</a></code>
method to support masking.</p>

<p>Args:
    inputs: Input tensor, or list/tuple of input tensors.
    <em>args: Additional positional arguments. Currently unused.
    *</em>kwargs: Additional keyword arguments. Currently unused.</p>

<p>Returns:
    A tensor or list/tuple of tensors.</p>
</div>


                            </div>
                            <div id="Mask.compute_output_shape" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#Mask.compute_output_shape">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">compute_output_shape</span><span class="signature">(self, input_shape)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">is</span> <span class="nb">tuple</span><span class="p">:</span>  <span class="c1"># true label provided</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]])</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># no true label provided</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]])</span>
</pre></div>

        </details>

            <div class="docstring"><p>Computes the output shape of the layer.</p>

<p>If the layer has not been built, this method will call <code><a href="#Mask.build">build</a></code> on the
layer. This assumes that the layer will later be used with inputs that
match the input shape provided here.</p>

<p>Args:
    input_shape: Shape tuple (tuple of integers)
        or list of shape tuples (one per output tensor of the layer).
        Shape tuples can include None for free dimensions,
        instead of an integer.</p>

<p>Returns:
    An input shape tuple.</p>
</div>


                            </div>
                            <div id="Mask.get_config" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#Mask.get_config">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">get_config</span><span class="signature">(self)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">Mask</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">config</span>
</pre></div>

        </details>

            <div class="docstring"><p>Returns the config of the layer.</p>

<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>

<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>

<p>Note that <code><a href="#Mask.get_config">get_config()</a></code> does not guarantee to return a fresh copy of dict
every time it is called. The callers should make a copy of the returned dict
if they want to modify it.</p>

<p>Returns:
    Python dictionary.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>keras.engine.base_layer.Layer</dt>
                                <dd id="Mask.__init__" class="function">Layer</dd>
                <dd id="Mask.build" class="function">build</dd>
                <dd id="Mask.add_weight" class="function">add_weight</dd>
                <dd id="Mask.from_config" class="function">from_config</dd>
                <dd id="Mask.compute_output_signature" class="function">compute_output_signature</dd>
                <dd id="Mask.compute_mask" class="function">compute_mask</dd>
                <dd id="Mask.dtype" class="variable">dtype</dd>
                <dd id="Mask.name" class="variable">name</dd>
                <dd id="Mask.supports_masking" class="variable">supports_masking</dd>
                <dd id="Mask.dynamic" class="variable">dynamic</dd>
                <dd id="Mask.stateful" class="variable">stateful</dd>
                <dd id="Mask.trainable" class="variable">trainable</dd>
                <dd id="Mask.activity_regularizer" class="variable">activity_regularizer</dd>
                <dd id="Mask.input_spec" class="variable">input_spec</dd>
                <dd id="Mask.trainable_weights" class="variable">trainable_weights</dd>
                <dd id="Mask.non_trainable_weights" class="variable">non_trainable_weights</dd>
                <dd id="Mask.weights" class="variable">weights</dd>
                <dd id="Mask.updates" class="variable">updates</dd>
                <dd id="Mask.losses" class="variable">losses</dd>
                <dd id="Mask.add_loss" class="function">add_loss</dd>
                <dd id="Mask.metrics" class="variable">metrics</dd>
                <dd id="Mask.add_metric" class="function">add_metric</dd>
                <dd id="Mask.add_update" class="function">add_update</dd>
                <dd id="Mask.set_weights" class="function">set_weights</dd>
                <dd id="Mask.get_weights" class="function">get_weights</dd>
                <dd id="Mask.get_updates_for" class="function">get_updates_for</dd>
                <dd id="Mask.get_losses_for" class="function">get_losses_for</dd>
                <dd id="Mask.get_input_mask_at" class="function">get_input_mask_at</dd>
                <dd id="Mask.get_output_mask_at" class="function">get_output_mask_at</dd>
                <dd id="Mask.input_mask" class="variable">input_mask</dd>
                <dd id="Mask.output_mask" class="variable">output_mask</dd>
                <dd id="Mask.get_input_shape_at" class="function">get_input_shape_at</dd>
                <dd id="Mask.get_output_shape_at" class="function">get_output_shape_at</dd>
                <dd id="Mask.get_input_at" class="function">get_input_at</dd>
                <dd id="Mask.get_output_at" class="function">get_output_at</dd>
                <dd id="Mask.input" class="variable">input</dd>
                <dd id="Mask.output" class="variable">output</dd>
                <dd id="Mask.input_shape" class="variable">input_shape</dd>
                <dd id="Mask.count_params" class="function">count_params</dd>
                <dd id="Mask.output_shape" class="variable">output_shape</dd>
                <dd id="Mask.inbound_nodes" class="variable">inbound_nodes</dd>
                <dd id="Mask.outbound_nodes" class="variable">outbound_nodes</dd>
                <dd id="Mask.apply" class="function">apply</dd>
                <dd id="Mask.add_variable" class="function">add_variable</dd>
                <dd id="Mask.variables" class="variable">variables</dd>
                <dd id="Mask.trainable_variables" class="variable">trainable_variables</dd>
                <dd id="Mask.non_trainable_variables" class="variable">non_trainable_variables</dd>
                <dd id="Mask.dtype_policy" class="variable">dtype_policy</dd>
                <dd id="Mask.compute_dtype" class="variable">compute_dtype</dd>
                <dd id="Mask.variable_dtype" class="variable">variable_dtype</dd>

            </div>
            <div><dt>tensorflow.python.module.module.Module</dt>
                                <dd id="Mask.name_scope" class="variable">name_scope</dd>
                <dd id="Mask.submodules" class="variable">submodules</dd>
                <dd id="Mask.with_name_scope" class="function">with_name_scope</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="squash">
                            <div class="attr function"><a class="headerlink" href="#squash">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">squash</span><span class="signature">(vectors, axis=-1)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">def</span> <span class="nf">squash</span><span class="p">(</span><span class="n">vectors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0</span>
<span class="sd">    :param vectors: some vectors to be squashed, N-dim tensor</span>
<span class="sd">    :param axis: the axis to squash</span>
<span class="sd">    :return: a Tensor with same shape as input vectors</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">s_squared_norm</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">vectors</span><span class="p">),</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">s_squared_norm</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">s_squared_norm</span><span class="p">)</span> <span class="o">/</span> <span class="n">K</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s_squared_norm</span> <span class="o">+</span> <span class="n">K</span><span class="o">.</span><span class="n">epsilon</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">vectors</span>
</pre></div>

        </details>

            <div class="docstring"><p>The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>vectors</strong>:  some vectors to be squashed, N-dim tensor</li>
<li><strong>axis</strong>:  the axis to squash</li>
</ul>

<h6 id="returns">Returns</h6>

<blockquote>
  <p>a Tensor with same shape as input vectors</p>
</blockquote>
</div>


                </section>
                <section id="margin_loss">
                            <div class="attr function"><a class="headerlink" href="#margin_loss">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">margin_loss</span><span class="signature">(y_true, y_pred)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">def</span> <span class="nf">margin_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>

    <span class="n">L</span> <span class="o">=</span> <span class="n">y_true</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.9</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">))</span> <span class="o">+</span> \
    <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">-</span> <span class="mf">0.1</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">K</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>

        </details>

    

                </section>
                <section id="plotfn">
                            <div class="attr function"><a class="headerlink" href="#plotfn">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">plotfn</span><span class="signature">(prob_P, prob_S)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">def</span> <span class="nf">plotfn</span><span class="p">(</span><span class="n">prob_P</span><span class="p">,</span><span class="n">prob_S</span><span class="p">):</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span> <span class="p">:</span> <span class="s1">&#39;normal&#39;</span><span class="p">,</span>
        <span class="s1">&#39;weight&#39;</span> <span class="p">:</span> <span class="s1">&#39;bold&#39;</span><span class="p">,</span>
        <span class="s1">&#39;size&#39;</span>   <span class="p">:</span> <span class="mi">16</span><span class="p">}</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">&#39;font&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">font</span><span class="p">)</span>

    <span class="n">yint</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">labf</span><span class="o">=</span><span class="n">prob_P</span>
    <span class="n">labfx</span><span class="o">=</span><span class="n">prob_S</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Output P-Probability&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="s1">&#39;large&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="c1">#ax1.plot(out,linewidth=2)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">labf</span><span class="p">,</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1">#ax1.set_xlim(0,len(out))</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">labf</span><span class="p">))</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1">#plt.yticks(yint)</span>


    <span class="n">ax2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">labfx</span><span class="p">,</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Window Index&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="s1">&#39;large&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Output S-Probability&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="s1">&#39;large&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="c1">#ax2.set_xlim(0,len(out))</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">labfx</span><span class="p">))</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

        </details>

    

                </section>
                <section id="butter_bandpass">
                            <div class="attr function"><a class="headerlink" href="#butter_bandpass">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">butter_bandpass</span><span class="signature">(lowcut, highcut, fs, order=5)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">def</span> <span class="nf">butter_bandpass</span><span class="p">(</span><span class="n">lowcut</span><span class="p">,</span> <span class="n">highcut</span><span class="p">,</span> <span class="n">fs</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">nyq</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">fs</span>
    <span class="n">low</span> <span class="o">=</span> <span class="n">lowcut</span> <span class="o">/</span> <span class="n">nyq</span>
    <span class="n">high</span> <span class="o">=</span> <span class="n">highcut</span> <span class="o">/</span> <span class="n">nyq</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="n">butter</span><span class="p">(</span><span class="n">order</span><span class="p">,</span> <span class="p">[</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">],</span> <span class="n">btype</span><span class="o">=</span><span class="s1">&#39;band&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">b</span><span class="p">,</span> <span class="n">a</span>
</pre></div>

        </details>

    

                </section>
                <section id="butter_bandpass_filter_zi">
                            <div class="attr function"><a class="headerlink" href="#butter_bandpass_filter_zi">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">butter_bandpass_filter_zi</span><span class="signature">(data, lowcut, highcut, fs, order=5)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">def</span> <span class="nf">butter_bandpass_filter_zi</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">lowcut</span><span class="p">,</span> <span class="n">highcut</span><span class="p">,</span> <span class="n">fs</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="n">butter_bandpass</span><span class="p">(</span><span class="n">lowcut</span><span class="p">,</span> <span class="n">highcut</span><span class="p">,</span> <span class="n">fs</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="n">order</span><span class="p">)</span>
    <span class="n">zi</span> <span class="o">=</span> <span class="n">lfilter_zi</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
    <span class="n">y</span><span class="p">,</span><span class="n">zo</span> <span class="o">=</span> <span class="n">lfilter</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">zi</span><span class="o">=</span><span class="n">zi</span><span class="o">*</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">y</span>
</pre></div>

        </details>

    

                </section>
                <section id="butter_bandpass_filter">
                            <div class="attr function"><a class="headerlink" href="#butter_bandpass_filter">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">butter_bandpass_filter</span><span class="signature">(data, lowcut, highcut, fs, order=5)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">def</span> <span class="nf">butter_bandpass_filter</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">lowcut</span><span class="p">,</span> <span class="n">highcut</span><span class="p">,</span> <span class="n">fs</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">nyq</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">fs</span>
    <span class="n">low</span> <span class="o">=</span> <span class="n">lowcut</span> <span class="o">/</span> <span class="n">nyq</span>
    <span class="n">high</span> <span class="o">=</span> <span class="n">highcut</span> <span class="o">/</span> <span class="n">nyq</span>

    <span class="n">b</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="n">butter</span><span class="p">(</span><span class="n">order</span><span class="p">,</span> <span class="p">[</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">],</span> <span class="n">btype</span><span class="o">=</span><span class="s1">&#39;band&#39;</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">lfilter</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>
</pre></div>

        </details>

    

                </section>
                <section id="sliding_window">
                            <div class="attr function"><a class="headerlink" href="#sliding_window">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">sliding_window</span><span class="signature">(data, size, stepsize=1, padded=False, axis=-1, copy=True)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">def</span> <span class="nf">sliding_window</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">stepsize</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padded</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate a sliding window over a signal</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    data : numpy array</span>
<span class="sd">        The array to be slided over.</span>
<span class="sd">    size : int</span>
<span class="sd">        The sliding window size</span>
<span class="sd">    stepsize : int</span>
<span class="sd">        The sliding window stepsize. Defaults to 1.</span>
<span class="sd">    axis : int</span>
<span class="sd">        The axis to slide over. Defaults to the last axis.</span>
<span class="sd">    copy : bool</span>
<span class="sd">        Return strided array as copy to avoid sideffects when manipulating the</span>
<span class="sd">        output array.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    data : numpy array</span>
<span class="sd">        A matrix where row in last dimension consists of one instance</span>
<span class="sd">        of the sliding window.</span>
<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    - Be wary of setting `copy` to `False` as undesired sideffects with the</span>
<span class="sd">      output values may occurr.</span>
<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; a = numpy.array([1, 2, 3, 4, 5])</span>
<span class="sd">    &gt;&gt;&gt; sliding_window(a, size=3)</span>
<span class="sd">    array([[1, 2, 3],</span>
<span class="sd">           [2, 3, 4],</span>
<span class="sd">           [3, 4, 5]])</span>
<span class="sd">    &gt;&gt;&gt; sliding_window(a, size=3, stepsize=2)</span>
<span class="sd">    array([[1, 2, 3],</span>
<span class="sd">           [3, 4, 5]])</span>
<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    pieces : Calculate number of pieces available by sliding</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">&gt;=</span> <span class="n">data</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Axis value out of range&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">stepsize</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Stepsize may not be zero or negative&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">size</span> <span class="o">&gt;</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Sliding window size may not exceed size of selected axis&quot;</span>
        <span class="p">)</span>

    <span class="n">shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">/</span> <span class="n">stepsize</span> <span class="o">-</span> <span class="n">size</span> <span class="o">/</span> <span class="n">stepsize</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>

    <span class="n">strides</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">strides</span><span class="p">)</span>
    <span class="n">strides</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">*=</span> <span class="n">stepsize</span>
    <span class="n">strides</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="n">axis</span><span class="p">])</span>

    <span class="n">strided</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">lib</span><span class="o">.</span><span class="n">stride_tricks</span><span class="o">.</span><span class="n">as_strided</span><span class="p">(</span>
        <span class="n">data</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="n">strides</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">copy</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">strided</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">strided</span>
</pre></div>

        </details>

            <div class="docstring"><p>Calculate a sliding window over a signal</p>

<h2 id="parameters">Parameters</h2>

<p>data : numpy array
    The array to be slided over.
size : int
    The sliding window size
stepsize : int
    The sliding window stepsize. Defaults to 1.
axis : int
    The axis to slide over. Defaults to the last axis.
copy : bool
    Return strided array as copy to avoid sideffects when manipulating the
    output array.</p>

<h2 id="returns">Returns</h2>

<p>data : numpy array
    A matrix where row in last dimension consists of one instance
    of the sliding window.</p>

<h2 id="notes">Notes</h2>

<ul>
<li>Be wary of setting <code>copy</code> to <code>False</code> as undesired sideffects with the
output values may occurr.
<h2 id="examples">Examples</h2></li>
</ul>

<div class="pdoc-code codehilite"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sliding_window</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="go">array([[1, 2, 3],</span>
<span class="go">       [2, 3, 4],</span>
<span class="go">       [3, 4, 5]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sliding_window</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stepsize</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="go">array([[1, 2, 3],</span>
<span class="go">       [3, 4, 5]])</span>
<span class="go">&lt;h2 id=&quot;see-also&quot;&gt;See Also&lt;/h2&gt;</span>
</code></pre></div>

<p>pieces : Calculate number of pieces available by sliding</p>
</div>


                </section>
                <section id="CapsuleLayer">
                                <div class="attr class">
        <a class="headerlink" href="#CapsuleLayer">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">CapsuleLayer</span><wbr>(<span class="base">keras.engine.base_layer.Layer</span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">class</span> <span class="nc">CapsuleLayer</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the </span>
<span class="sd">    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron</span>
<span class="sd">    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \</span>
<span class="sd">    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.</span>
<span class="sd">    </span>
<span class="sd">    :param num_capsule: number of capsules in this layer</span>
<span class="sd">    :param dim_capsule: dimension of the output vectors of the capsules in this layer</span>
<span class="sd">    :param routings: number of iterations for the routing algorithm</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_capsule</span><span class="p">,</span> <span class="n">dim_capsule</span><span class="p">,</span> <span class="n">routings</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                 <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;glorot_uniform&#39;</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CapsuleLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_capsule</span> <span class="o">=</span> <span class="n">num_capsule</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim_capsule</span> <span class="o">=</span> <span class="n">dim_capsule</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">routings</span> <span class="o">=</span> <span class="n">routings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_initializer</span> <span class="o">=</span> <span class="n">initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">kernel_initializer</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_num_capsule</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim_capsule</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

        <span class="c1"># Transform matrix</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_capsule</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_num_capsule</span><span class="p">,</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">dim_capsule</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim_capsule</span><span class="p">],</span>
                                 <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_initializer</span><span class="p">,</span>
                                 <span class="n">name</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># inputs.shape=[None, input_num_capsule, input_dim_capsule]</span>
        <span class="c1"># inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]</span>
        <span class="n">inputs_expand</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Replicate num_capsule dimension to prepare being multiplied by W</span>
        <span class="c1"># inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule]</span>
        <span class="n">inputs_tiled</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">inputs_expand</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_capsule</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

        <span class="c1"># Compute `inputs * W` by scanning inputs_tiled on dimension 0.</span>
        <span class="c1"># x.shape=[num_capsule, input_num_capsule, input_dim_capsule]</span>
        <span class="c1"># W.shape=[num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]</span>
        <span class="c1"># Regard the first two dimensions as `batch` dimension,</span>
        <span class="c1"># then matmul: [input_dim_capsule] x [dim_capsule, input_dim_capsule]^T -&gt; [dim_capsule].</span>
        <span class="c1"># inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]</span>
        <span class="n">inputs_hat</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">map_fn</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">own_batch_dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">elems</span><span class="o">=</span><span class="n">inputs_tiled</span><span class="p">)</span>

        <span class="c1"># Begin: Routing algorithm ---------------------------------------------------------------------#</span>
        <span class="c1"># The prior for coupling coefficient, initialized as zeros.</span>
        <span class="c1"># b.shape = [None, self.num_capsule, self.input_num_capsule].</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs_hat</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_capsule</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_num_capsule</span><span class="p">])</span>

        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">routings</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;The routings should be &gt; 0.&#39;</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">routings</span><span class="p">):</span>
            <span class="c1"># c.shape=[batch_size, num_capsule, input_num_capsule]</span>
            <span class="n">c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># c.shape =  [batch_size, num_capsule, input_num_capsule]</span>
            <span class="c1"># inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]</span>
            <span class="c1"># The first two dimensions as `batch` dimension,</span>
            <span class="c1"># then matmal: [input_num_capsule] x [input_num_capsule, dim_capsule] -&gt; [dim_capsule].</span>
            <span class="c1"># outputs.shape=[None, num_capsule, dim_capsule]</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">squash</span><span class="p">(</span><span class="n">own_batch_dot</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">inputs_hat</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))</span>  <span class="c1"># [None, 10, 16]</span>
			<span class="c1">#print(c,inputs_hat,outputs)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">routings</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># outputs.shape =  [None, num_capsule, dim_capsule]</span>
                <span class="c1"># inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]</span>
                <span class="c1"># The first two dimensions as `batch` dimension,</span>
                <span class="c1"># then matmal: [dim_capsule] x [input_num_capsule, dim_capsule]^T -&gt; [input_num_capsule].</span>
                <span class="c1"># b.shape=[batch_size, num_capsule, input_num_capsule]</span>
                <span class="n">b</span> <span class="o">+=</span> <span class="n">own_batch_dot</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">inputs_hat</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
        <span class="c1"># End: Routing algorithm -----------------------------------------------------------------------#</span>

        <span class="k">return</span> <span class="n">outputs</span>

    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_capsule</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_capsule</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;num_capsule&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_capsule</span><span class="p">,</span>
            <span class="s1">&#39;dim_capsule&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_capsule</span><span class="p">,</span>
            <span class="s1">&#39;routings&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">routings</span>
        <span class="p">}</span>
        <span class="n">base_config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">CapsuleLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">base_config</span><span class="o">.</span><span class="n">items</span><span class="p">())</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">items</span><span class="p">()))</span>
</pre></div>

        </details>

            <div class="docstring"><p>The capsule layer. It is similar to Dense layer. Dense layer has <code>in_num</code> inputs, each is a scalar, the output of the 
neuron from the former layer, and it has <code>out_num</code> output neurons. CapsuleLayer just expand the output of the neuron
from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape =     [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>num_capsule</strong>:  number of capsules in this layer</li>
<li><strong>dim_capsule</strong>:  dimension of the output vectors of the capsules in this layer</li>
<li><strong>routings</strong>:  number of iterations for the routing algorithm</li>
</ul>
</div>


                            <div id="CapsuleLayer.__init__" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#CapsuleLayer.__init__">#&nbsp;&nbsp</a>

        
            <span class="name">CapsuleLayer</span><span class="signature">(
    num_capsule,
    dim_capsule,
    routings=3,
    kernel_initializer=&#39;glorot_uniform&#39;,
    **kwargs
)</span>
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_capsule</span><span class="p">,</span> <span class="n">dim_capsule</span><span class="p">,</span> <span class="n">routings</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                 <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;glorot_uniform&#39;</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CapsuleLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_capsule</span> <span class="o">=</span> <span class="n">num_capsule</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim_capsule</span> <span class="o">=</span> <span class="n">dim_capsule</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">routings</span> <span class="o">=</span> <span class="n">routings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_initializer</span> <span class="o">=</span> <span class="n">initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">kernel_initializer</span><span class="p">)</span>
</pre></div>

        </details>

    

                            </div>
                            <div id="CapsuleLayer.build" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#CapsuleLayer.build">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">build</span><span class="signature">(self, input_shape)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_num_capsule</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim_capsule</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

        <span class="c1"># Transform matrix</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_capsule</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_num_capsule</span><span class="p">,</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">dim_capsule</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim_capsule</span><span class="p">],</span>
                                 <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_initializer</span><span class="p">,</span>
                                 <span class="n">name</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>

        </details>

            <div class="docstring"><p>Creates the variables of the layer (optional, for subclass implementers).</p>

<p>This is a method that implementers of subclasses of <code>Layer</code> or <code>Model</code>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>

<p>This is typically used to create the weights of <code>Layer</code> subclasses.</p>

<p>Args:
  input_shape: Instance of <code>TensorShape</code>, or list of instances of
    <code>TensorShape</code> if the layer expects a list of inputs
    (one instance per input).</p>
</div>


                            </div>
                            <div id="CapsuleLayer.call" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#CapsuleLayer.call">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">call</span><span class="signature">(self, inputs, training=None)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># inputs.shape=[None, input_num_capsule, input_dim_capsule]</span>
        <span class="c1"># inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]</span>
        <span class="n">inputs_expand</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Replicate num_capsule dimension to prepare being multiplied by W</span>
        <span class="c1"># inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule]</span>
        <span class="n">inputs_tiled</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">inputs_expand</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_capsule</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

        <span class="c1"># Compute `inputs * W` by scanning inputs_tiled on dimension 0.</span>
        <span class="c1"># x.shape=[num_capsule, input_num_capsule, input_dim_capsule]</span>
        <span class="c1"># W.shape=[num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]</span>
        <span class="c1"># Regard the first two dimensions as `batch` dimension,</span>
        <span class="c1"># then matmul: [input_dim_capsule] x [dim_capsule, input_dim_capsule]^T -&gt; [dim_capsule].</span>
        <span class="c1"># inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]</span>
        <span class="n">inputs_hat</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">map_fn</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">own_batch_dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">elems</span><span class="o">=</span><span class="n">inputs_tiled</span><span class="p">)</span>

        <span class="c1"># Begin: Routing algorithm ---------------------------------------------------------------------#</span>
        <span class="c1"># The prior for coupling coefficient, initialized as zeros.</span>
        <span class="c1"># b.shape = [None, self.num_capsule, self.input_num_capsule].</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs_hat</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_capsule</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_num_capsule</span><span class="p">])</span>

        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">routings</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;The routings should be &gt; 0.&#39;</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">routings</span><span class="p">):</span>
            <span class="c1"># c.shape=[batch_size, num_capsule, input_num_capsule]</span>
            <span class="n">c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># c.shape =  [batch_size, num_capsule, input_num_capsule]</span>
            <span class="c1"># inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]</span>
            <span class="c1"># The first two dimensions as `batch` dimension,</span>
            <span class="c1"># then matmal: [input_num_capsule] x [input_num_capsule, dim_capsule] -&gt; [dim_capsule].</span>
            <span class="c1"># outputs.shape=[None, num_capsule, dim_capsule]</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">squash</span><span class="p">(</span><span class="n">own_batch_dot</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">inputs_hat</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))</span>  <span class="c1"># [None, 10, 16]</span>
			<span class="c1">#print(c,inputs_hat,outputs)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">routings</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># outputs.shape =  [None, num_capsule, dim_capsule]</span>
                <span class="c1"># inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]</span>
                <span class="c1"># The first two dimensions as `batch` dimension,</span>
                <span class="c1"># then matmal: [dim_capsule] x [input_num_capsule, dim_capsule]^T -&gt; [input_num_capsule].</span>
                <span class="c1"># b.shape=[batch_size, num_capsule, input_num_capsule]</span>
                <span class="n">b</span> <span class="o">+=</span> <span class="n">own_batch_dot</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">inputs_hat</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
        <span class="c1"># End: Routing algorithm -----------------------------------------------------------------------#</span>

        <span class="k">return</span> <span class="n">outputs</span>
</pre></div>

        </details>

            <div class="docstring"><p>This is where the layer's logic lives.</p>

<p>Note here that <code><a href="#CapsuleLayer.call">call()</a></code> method in <code>tf.keras</code> is little bit different
from <code>keras</code> API. In <code>keras</code> API, you can pass support masking for
layers as additional arguments. Whereas <code>tf.keras</code> has <code><a href="#CapsuleLayer.compute_mask">compute_mask()</a></code>
method to support masking.</p>

<p>Args:
    inputs: Input tensor, or list/tuple of input tensors.
    <em>args: Additional positional arguments. Currently unused.
    *</em>kwargs: Additional keyword arguments. Currently unused.</p>

<p>Returns:
    A tensor or list/tuple of tensors.</p>
</div>


                            </div>
                            <div id="CapsuleLayer.compute_output_shape" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#CapsuleLayer.compute_output_shape">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">compute_output_shape</span><span class="signature">(self, input_shape)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_capsule</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_capsule</span><span class="p">])</span>
</pre></div>

        </details>

            <div class="docstring"><p>Computes the output shape of the layer.</p>

<p>If the layer has not been built, this method will call <code><a href="#CapsuleLayer.build">build</a></code> on the
layer. This assumes that the layer will later be used with inputs that
match the input shape provided here.</p>

<p>Args:
    input_shape: Shape tuple (tuple of integers)
        or list of shape tuples (one per output tensor of the layer).
        Shape tuples can include None for free dimensions,
        instead of an integer.</p>

<p>Returns:
    An input shape tuple.</p>
</div>


                            </div>
                            <div id="CapsuleLayer.get_config" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#CapsuleLayer.get_config">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">get_config</span><span class="signature">(self)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;num_capsule&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_capsule</span><span class="p">,</span>
            <span class="s1">&#39;dim_capsule&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_capsule</span><span class="p">,</span>
            <span class="s1">&#39;routings&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">routings</span>
        <span class="p">}</span>
        <span class="n">base_config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">CapsuleLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">base_config</span><span class="o">.</span><span class="n">items</span><span class="p">())</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">items</span><span class="p">()))</span>
</pre></div>

        </details>

            <div class="docstring"><p>Returns the config of the layer.</p>

<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>

<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>

<p>Note that <code><a href="#CapsuleLayer.get_config">get_config()</a></code> does not guarantee to return a fresh copy of dict
every time it is called. The callers should make a copy of the returned dict
if they want to modify it.</p>

<p>Returns:
    Python dictionary.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>keras.engine.base_layer.Layer</dt>
                                <dd id="CapsuleLayer.add_weight" class="function">add_weight</dd>
                <dd id="CapsuleLayer.from_config" class="function">from_config</dd>
                <dd id="CapsuleLayer.compute_output_signature" class="function">compute_output_signature</dd>
                <dd id="CapsuleLayer.compute_mask" class="function">compute_mask</dd>
                <dd id="CapsuleLayer.dtype" class="variable">dtype</dd>
                <dd id="CapsuleLayer.name" class="variable">name</dd>
                <dd id="CapsuleLayer.supports_masking" class="variable">supports_masking</dd>
                <dd id="CapsuleLayer.dynamic" class="variable">dynamic</dd>
                <dd id="CapsuleLayer.stateful" class="variable">stateful</dd>
                <dd id="CapsuleLayer.trainable" class="variable">trainable</dd>
                <dd id="CapsuleLayer.activity_regularizer" class="variable">activity_regularizer</dd>
                <dd id="CapsuleLayer.input_spec" class="variable">input_spec</dd>
                <dd id="CapsuleLayer.trainable_weights" class="variable">trainable_weights</dd>
                <dd id="CapsuleLayer.non_trainable_weights" class="variable">non_trainable_weights</dd>
                <dd id="CapsuleLayer.weights" class="variable">weights</dd>
                <dd id="CapsuleLayer.updates" class="variable">updates</dd>
                <dd id="CapsuleLayer.losses" class="variable">losses</dd>
                <dd id="CapsuleLayer.add_loss" class="function">add_loss</dd>
                <dd id="CapsuleLayer.metrics" class="variable">metrics</dd>
                <dd id="CapsuleLayer.add_metric" class="function">add_metric</dd>
                <dd id="CapsuleLayer.add_update" class="function">add_update</dd>
                <dd id="CapsuleLayer.set_weights" class="function">set_weights</dd>
                <dd id="CapsuleLayer.get_weights" class="function">get_weights</dd>
                <dd id="CapsuleLayer.get_updates_for" class="function">get_updates_for</dd>
                <dd id="CapsuleLayer.get_losses_for" class="function">get_losses_for</dd>
                <dd id="CapsuleLayer.get_input_mask_at" class="function">get_input_mask_at</dd>
                <dd id="CapsuleLayer.get_output_mask_at" class="function">get_output_mask_at</dd>
                <dd id="CapsuleLayer.input_mask" class="variable">input_mask</dd>
                <dd id="CapsuleLayer.output_mask" class="variable">output_mask</dd>
                <dd id="CapsuleLayer.get_input_shape_at" class="function">get_input_shape_at</dd>
                <dd id="CapsuleLayer.get_output_shape_at" class="function">get_output_shape_at</dd>
                <dd id="CapsuleLayer.get_input_at" class="function">get_input_at</dd>
                <dd id="CapsuleLayer.get_output_at" class="function">get_output_at</dd>
                <dd id="CapsuleLayer.input" class="variable">input</dd>
                <dd id="CapsuleLayer.output" class="variable">output</dd>
                <dd id="CapsuleLayer.input_shape" class="variable">input_shape</dd>
                <dd id="CapsuleLayer.count_params" class="function">count_params</dd>
                <dd id="CapsuleLayer.output_shape" class="variable">output_shape</dd>
                <dd id="CapsuleLayer.inbound_nodes" class="variable">inbound_nodes</dd>
                <dd id="CapsuleLayer.outbound_nodes" class="variable">outbound_nodes</dd>
                <dd id="CapsuleLayer.apply" class="function">apply</dd>
                <dd id="CapsuleLayer.add_variable" class="function">add_variable</dd>
                <dd id="CapsuleLayer.variables" class="variable">variables</dd>
                <dd id="CapsuleLayer.trainable_variables" class="variable">trainable_variables</dd>
                <dd id="CapsuleLayer.non_trainable_variables" class="variable">non_trainable_variables</dd>
                <dd id="CapsuleLayer.dtype_policy" class="variable">dtype_policy</dd>
                <dd id="CapsuleLayer.compute_dtype" class="variable">compute_dtype</dd>
                <dd id="CapsuleLayer.variable_dtype" class="variable">variable_dtype</dd>

            </div>
            <div><dt>tensorflow.python.module.module.Module</dt>
                                <dd id="CapsuleLayer.name_scope" class="variable">name_scope</dd>
                <dd id="CapsuleLayer.submodules" class="variable">submodules</dd>
                <dd id="CapsuleLayer.with_name_scope" class="function">with_name_scope</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="PrimaryCap">
                            <div class="attr function"><a class="headerlink" href="#PrimaryCap">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">PrimaryCap</span><span class="signature">(inputs, dim_capsule, n_channels, kernel_size, strides, padding)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">def</span> <span class="nf">PrimaryCap</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">dim_capsule</span><span class="p">,</span> <span class="n">n_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">padding</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Apply Conv2D `n_channels` times and concatenate all capsules</span>
<span class="sd">    :param inputs: 4D tensor, shape=[None, width, height, channels]</span>
<span class="sd">    :param dim_capsule: the dim of the output vector of capsule</span>
<span class="sd">    :param n_channels: the number of types of capsules</span>
<span class="sd">    :return: output tensor, shape=[None, num_capsule, dim_capsule]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="n">dim_capsule</span><span class="o">*</span><span class="n">n_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                           <span class="n">name</span><span class="o">=</span><span class="s1">&#39;primarycap_conv2d&#39;</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Reshape</span><span class="p">(</span><span class="n">target_shape</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim_capsule</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;primarycap_reshape&#39;</span><span class="p">)(</span><span class="n">output</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">layers</span><span class="o">.</span><span class="n">Lambda</span><span class="p">(</span><span class="n">squash</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;primarycap_squash&#39;</span><span class="p">)(</span><span class="n">outputs</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Apply Conv2D <code>n_channels</code> times and concatenate all capsules</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>inputs</strong>:  4D tensor, shape=[None, width, height, channels]</li>
<li><strong>dim_capsule</strong>:  the dim of the output vector of capsule</li>
<li><strong>n_channels</strong>:  the number of types of capsules</li>
</ul>

<h6 id="returns">Returns</h6>

<blockquote>
  <p>output tensor, shape=[None, num_capsule, dim_capsule]</p>
</blockquote>
</div>


                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.type) {
                    case "function":
                        heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span><span class="signature">${doc.signature}:</span>`;
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value">${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.type}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>